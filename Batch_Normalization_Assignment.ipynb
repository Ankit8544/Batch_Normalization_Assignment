{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`Q.No-01`    Theory and Concepts :**\n",
    "\n",
    "1. **Explain the concept of batch normalization in the context of Artificial Neural Networks.**\n",
    "\n",
    "2. **Describe the benefits of using batch normalization during training.**\n",
    "\n",
    "3. **Discuss the working principle of batch normalization, including the normalization step and the learnable parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch normalization is a technique used in artificial neural networks to improve the training process and enhance the model's performance. Here are the key points explaining the concept of batch normalization :**\n",
    "\n",
    "-   **What is Batch Normalization?**\n",
    "\n",
    "      -   **Batch normalization** is a process that normalizes the inputs of each layer in a neural network. It standardizes the inputs by re-centering and re-scaling them for each mini-batch during training. This helps stabilize and accelerate the training process.\n",
    "\n",
    "-   **Why Use Batch Normalization?**\n",
    "\n",
    "      1. **Stabilizes Learning**: By normalizing the inputs, batch normalization reduces internal covariate shift, making the learning process more stable.\n",
    "      \n",
    "      2. **Accelerates Training**: It allows the use of higher learning rates by reducing the risk of divergence during training.\n",
    "      \n",
    "      3. **Regularization Effect**: It has a slight regularizing effect, reducing the need for other forms of regularization such as dropout.\n",
    "      \n",
    "      4. **Reduces Dependence on Initialization**: Models become less sensitive to the initialization of parameters, making it easier to train deep networks.\n",
    "\n",
    "-   **How Does Batch Normalization Work?**\n",
    "\n",
    "      -   For each mini-batch during training:\n",
    "            \n",
    "            1. **Calculate Mean and Variance**: Compute the mean ($\\mu_B$) and variance ($\\sigma_B^2$) of the mini-batch.\n",
    "            \n",
    "            2. **Normalize**: Normalize the inputs using the mean and variance:\n",
    "\n",
    "            $$\\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "            \n",
    "            where $x^{(i)}$ is an input, $\\epsilon$ is a small constant to prevent division by zero.\n",
    "            \n",
    "            3. **Scale and Shift**: Apply scaling ($\\gamma$) and shifting ($\\beta$) parameters:\n",
    "            \n",
    "            $$y^{(i)} = \\gamma \\hat{x}^{(i)} + \\beta$$\n",
    "            \n",
    "            These parameters are learned during training.\n",
    "\n",
    "-   **Integration in Neural Networks**\n",
    "\n",
    "      -   Batch normalization can be applied to the inputs of each layer. In a typical neural network, it is often applied after the linear transformation (affine transformation) and before the activation function. Here’s how it fits into a layer:\n",
    "      \n",
    "            1. **Linear Transformation**: $ Z = W \\cdot X + b $\n",
    "            \n",
    "            2. **Batch Normalization**: $ \\hat{Z} = \\frac{Z - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} $\n",
    "            \n",
    "            3. **Scaling and Shifting**: $ \\hat{Z} = \\gamma \\hat{Z} + \\beta $\n",
    "            \n",
    "            4. **Activation Function**: $ A = f(\\hat{Z}) $\n",
    "\n",
    "-   **Benefits of Batch Normalization**\n",
    "\n",
    "      1. **Improved Gradient Flow**: By maintaining consistent distributions of activations, it helps gradients flow more easily through the network.\n",
    "      \n",
    "      2. **Reduced Training Time**: Enables the use of higher learning rates and accelerates convergence.\n",
    "      \n",
    "      3. **Increased Robustness**: Reduces sensitivity to the initialization of weights and hyperparameters.\n",
    "\n",
    "-   **Batch Normalization During Inference**\n",
    "\n",
    "      -   During inference, batch normalization uses the population statistics (mean and variance) rather than the mini-batch statistics. These population statistics are typically computed as a moving average during training.\n",
    "\n",
    "-   **Conclusion**\n",
    "\n",
    "      -   Batch normalization is a powerful technique that improves the training process of neural networks by normalizing layer inputs, accelerating training, and providing some regularization benefits. It has become a standard practice in building deep learning models due to its significant impact on performance and training stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch normalization provides several key benefits during the training of artificial neural networks, contributing to improved performance and efficiency. Here are the main advantages :**\n",
    "\n",
    "1. **Stabilizes and Accelerates Training**\n",
    "\n",
    "    -    **Stabilized Learning Process**: By normalizing the inputs of each layer, batch normalization reduces the internal covariate shift, where the distribution of layer inputs changes during training. This stabilization helps the network learn more efficiently.\n",
    "\n",
    "    -    **Faster Convergence**: With more stable learning, batch normalization allows the use of higher learning rates, which can speed up the convergence of the training process. Higher learning rates help the optimizer to make larger updates, accelerating the training.\n",
    "\n",
    "2. **Reduces Sensitivity to Initialization**\n",
    "\n",
    "    -    **Less Sensitive to Weight Initialization**: Neural networks can be very sensitive to the initial values of weights. Batch normalization mitigates this sensitivity, making the network less dependent on the precise initialization of weights. This can simplify the process of setting up a neural network.\n",
    "\n",
    "3. **Provides Regularization Effect**\n",
    "\n",
    "    -    **Reduces Overfitting**: While not a substitute for explicit regularization techniques like dropout, batch normalization introduces some noise due to mini-batch statistics, which can have a regularizing effect. This noise can help prevent overfitting to some extent.\n",
    "\n",
    "4. **Improves Gradient Flow**\n",
    "\n",
    "    -    **Enhanced Gradient Flow**: Normalizing the inputs helps in maintaining consistent distributions of activations, which improves the gradient flow through the network. This mitigates issues like vanishing or exploding gradients, especially in deep networks.\n",
    "\n",
    "5. **Allows for Use of Higher Learning Rates**\n",
    "\n",
    "    -    **Higher Learning Rates**: With reduced internal covariate shift and stabilized gradients, higher learning rates can be safely used. This can significantly speed up the training process.\n",
    "\n",
    "6. **Reduces Need for Other Forms of Regularization**\n",
    "\n",
    "    -    **Potentially Less Need for Dropout**: While not a direct replacement, the regularizing effect of batch normalization can sometimes reduce the need for other regularization techniques like dropout, simplifying the network design.\n",
    "\n",
    "7. **Consistency Across Mini-Batches**\n",
    "\n",
    "    -    **Consistent Layer Behavior**: Batch normalization helps ensure that the distribution of inputs to each layer remains consistent across different mini-batches, leading to more predictable and reliable training dynamics.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "-    In summary, batch normalization offers several benefits during training:\n",
    "\n",
    "        1. **Stabilizes and accelerates the learning process**.\n",
    "        \n",
    "        2. **Reduces sensitivity to weight initialization**.\n",
    "        \n",
    "        3. **Provides a regularization effect**.\n",
    "        \n",
    "        4. **Improves gradient flow through the network**.\n",
    "        \n",
    "        5. **Enables the use of higher learning rates**.\n",
    "        \n",
    "        6. **Potentially reduces the need for other regularization techniques**.\n",
    "        \n",
    "        7. **Ensures consistent layer behavior across mini-batches**.\n",
    "\n",
    "**These benefits make batch normalization a powerful and widely-used technique in training artificial neural networks, particularly deep networks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch normalization works by normalizing the inputs of each layer within a neural network. This process involves a few key steps: calculating the mean and variance of the inputs, normalizing the inputs, and then applying learnable scale and shift parameters.**\n",
    "\n",
    "**Here is a detailed explanation of each step :**\n",
    "\n",
    "-    **Working Principle of Batch Normalization**\n",
    "\n",
    "        1. Calculating Mean and Variance\n",
    "\n",
    "            For each mini-batch during training, batch normalization first computes the mean and variance of the inputs. Let $ x^{(i)} $ represent the input of the $ i $-th neuron in the layer for a given mini-batch:\n",
    "\n",
    "            - **Mean**: \n",
    "            \n",
    "            $$ \\mu_B = \\frac{1}{m} \\sum_{i=1}^m x^{(i)} $$\n",
    "            \n",
    "            where \\( m \\) is the number of inputs in the mini-batch.\n",
    "\n",
    "            - **Variance**:\n",
    "            \n",
    "            $$ \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)} - \\mu_B)^2 $$\n",
    "\n",
    "        2. **Normalizing the Inputs**\n",
    "\n",
    "            Next, the inputs are normalized using the computed mean and variance:\n",
    "\n",
    "            - **Normalization**:\n",
    "\n",
    "              $$ \\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} $$\n",
    "            \n",
    "              where $ \\epsilon $ is a small constant added to the variance to prevent division by zero and ensure numerical stability.\n",
    "\n",
    "        3. **Applying Learnable Scale and Shift Parameters**\n",
    "\n",
    "            After normalization, batch normalization applies learnable parameters to allow the network to scale and shift the normalized values. This ensures that the network can still represent the necessary transformations even after normalization:\n",
    "\n",
    "            - **Scale and Shift**:\n",
    "              \n",
    "              $$ y^{(i)} = \\gamma \\hat{x}^{(i)} + \\beta $$\n",
    "\n",
    "              where $ \\gamma $ and $ \\beta $ are learnable parameters that allow the model to scale and shift the normalized inputs.\n",
    "\n",
    "-    **Summary of the Process**\n",
    "\n",
    "      The entire process can be summarized as follows:\n",
    "\n",
    "        1. **Compute the mean and variance** of the inputs for the current mini-batch.\n",
    "        2. **Normalize the inputs** using the computed mean and variance to have zero mean and unit variance.\n",
    "        3. **Scale and shift** the normalized inputs using the learnable parameters $ \\gamma $ and $ \\beta $.\n",
    "\n",
    "-    **Learnable Parameters: $ \\gamma $ and $ \\beta $**\n",
    "\n",
    "      - **$ \\gamma $ (Scale Parameter)**: This parameter allows the network to scale the normalized inputs. If $ \\gamma $ is set to 1, the scaling has no effect.\n",
    "      \n",
    "      - **$ \\beta $ (Shift Parameter)**: This parameter allows the network to shift the normalized inputs. If $ \\beta $ is set to 0, the shifting has no effect.\n",
    "\n",
    "      These parameters are learned during the training process along with the other parameters of the network. They provide the flexibility needed for the network to learn the appropriate transformations even after normalization.\n",
    "\n",
    "-    **Batch Normalization During Inference**\n",
    "\n",
    "      During inference (i.e., when making predictions with the trained model), the mean and variance are computed differently. Instead of using mini-batch statistics, batch normalization uses running estimates of the mean and variance accumulated during training:\n",
    "\n",
    "      - **Population Mean and Variance**: The running estimates are typically computed as an exponential moving average of the mini-batch statistics observed during training.\n",
    "\n",
    "-    **Conclusion**\n",
    "\n",
    "      Batch normalization improves the training of neural networks by normalizing the inputs of each layer, thus reducing internal covariate shift and allowing for faster and more stable training. The key steps include calculating the mini-batch mean and variance, normalizing the inputs, and applying learnable scale ($ \\gamma $) and shift ($ \\beta $) parameters to retain the network's capacity to learn the necessary transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`Q.No-02`    Implementation :**\n",
    "\n",
    "1. **Choose a dataset of your choice (e.g., MNIST, CIFAR-10) and preprocess it.**\n",
    "\n",
    "2. **Implement a simple feedforward neural network using any deep learning framework/library (e.g., TensorFlow, PyTorch).**\n",
    "\n",
    "3. **Train the neural network on the chosen dataset without using batch normalization.**\n",
    "\n",
    "4. **Implement batch normalization layers in the neural network and train the model again.**\n",
    "\n",
    "5. **Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization.**\n",
    "\n",
    "6. **Discuss the impact of batch normalization on the training process and the performance of the neural network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Preprocess the CIFAR-10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [01:04<00:00, 2656446.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transform to normalize the data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Load the CIFAR-10 training and test datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Implement a Simple Feedforward Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # The input layer takes the flattened image of size 32*32*3 (CIFAR-10 images are 32x32 pixels with 3 channels for RGB).\n",
    "        # 512 hidden units are chosen to capture a reasonable amount of complexity from the input.\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        \n",
    "        # 256 hidden units in the second layer reduce the dimensionality while still maintaining sufficient capacity for complex patterns.\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        \n",
    "        # The output layer has 10 units corresponding to the 10 classes in CIFAR-10 (airplane, car, bird, etc.).\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Flatten the image into a 1D vector before feeding it into the fully connected layers.\n",
    "        x = x.view(-1, 32*32*3)\n",
    "        \n",
    "        # Apply ReLU activation after each hidden layer to introduce non-linearity.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # No activation function on the final layer since we will apply softmax in the loss function (cross-entropy loss).\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the simple neural network.\n",
    "net = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Train the Neural Network Without Batch Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 100] loss: 1.904\n",
      "[Epoch 1, Batch 200] loss: 1.673\n",
      "[Epoch 1, Batch 300] loss: 1.620\n",
      "[Epoch 1, Batch 400] loss: 1.568\n",
      "[Epoch 1, Batch 500] loss: 1.535\n",
      "Epoch 1, Learning Rate: [0.01]\n",
      "[Epoch 2, Batch 100] loss: 1.437\n",
      "[Epoch 2, Batch 200] loss: 1.444\n",
      "[Epoch 2, Batch 300] loss: 1.408\n",
      "[Epoch 2, Batch 400] loss: 1.424\n",
      "[Epoch 2, Batch 500] loss: 1.400\n",
      "Epoch 2, Learning Rate: [0.01]\n",
      "[Epoch 3, Batch 100] loss: 1.321\n",
      "[Epoch 3, Batch 200] loss: 1.319\n",
      "[Epoch 3, Batch 300] loss: 1.317\n",
      "[Epoch 3, Batch 400] loss: 1.312\n",
      "[Epoch 3, Batch 500] loss: 1.288\n",
      "Epoch 3, Learning Rate: [0.01]\n",
      "[Epoch 4, Batch 100] loss: 1.202\n",
      "[Epoch 4, Batch 200] loss: 1.209\n",
      "[Epoch 4, Batch 300] loss: 1.239\n",
      "[Epoch 4, Batch 400] loss: 1.226\n",
      "[Epoch 4, Batch 500] loss: 1.234\n",
      "Epoch 4, Learning Rate: [0.01]\n",
      "[Epoch 5, Batch 100] loss: 1.107\n",
      "[Epoch 5, Batch 200] loss: 1.124\n",
      "[Epoch 5, Batch 300] loss: 1.154\n",
      "[Epoch 5, Batch 400] loss: 1.150\n",
      "[Epoch 5, Batch 500] loss: 1.161\n",
      "Epoch 5, Learning Rate: [0.001]\n",
      "[Epoch 6, Batch 100] loss: 0.949\n",
      "[Epoch 6, Batch 200] loss: 0.920\n",
      "[Epoch 6, Batch 300] loss: 0.905\n",
      "[Epoch 6, Batch 400] loss: 0.889\n",
      "[Epoch 6, Batch 500] loss: 0.886\n",
      "Epoch 6, Learning Rate: [0.001]\n",
      "[Epoch 7, Batch 100] loss: 0.835\n",
      "[Epoch 7, Batch 200] loss: 0.850\n",
      "[Epoch 7, Batch 300] loss: 0.852\n",
      "[Epoch 7, Batch 400] loss: 0.844\n",
      "[Epoch 7, Batch 500] loss: 0.840\n",
      "Epoch 7, Learning Rate: [0.001]\n",
      "[Epoch 8, Batch 100] loss: 0.810\n",
      "[Epoch 8, Batch 200] loss: 0.805\n",
      "[Epoch 8, Batch 300] loss: 0.811\n",
      "[Epoch 8, Batch 400] loss: 0.817\n",
      "[Epoch 8, Batch 500] loss: 0.804\n",
      "Epoch 8, Learning Rate: [0.001]\n",
      "[Epoch 9, Batch 100] loss: 0.772\n",
      "[Epoch 9, Batch 200] loss: 0.780\n",
      "[Epoch 9, Batch 300] loss: 0.775\n",
      "[Epoch 9, Batch 400] loss: 0.794\n",
      "[Epoch 9, Batch 500] loss: 0.785\n",
      "Epoch 9, Learning Rate: [0.001]\n",
      "[Epoch 10, Batch 100] loss: 0.727\n",
      "[Epoch 10, Batch 200] loss: 0.752\n",
      "[Epoch 10, Batch 300] loss: 0.768\n",
      "[Epoch 10, Batch 400] loss: 0.767\n",
      "[Epoch 10, Batch 500] loss: 0.763\n",
      "Epoch 10, Learning Rate: [0.0001]\n",
      "[Epoch 11, Batch 100] loss: 0.714\n",
      "[Epoch 11, Batch 200] loss: 0.696\n",
      "[Epoch 11, Batch 300] loss: 0.716\n",
      "[Epoch 11, Batch 400] loss: 0.716\n",
      "[Epoch 11, Batch 500] loss: 0.710\n",
      "Epoch 11, Learning Rate: [0.0001]\n",
      "[Epoch 12, Batch 100] loss: 0.710\n",
      "[Epoch 12, Batch 200] loss: 0.699\n",
      "[Epoch 12, Batch 300] loss: 0.718\n",
      "[Epoch 12, Batch 400] loss: 0.691\n",
      "[Epoch 12, Batch 500] loss: 0.699\n",
      "Epoch 12, Learning Rate: [0.0001]\n",
      "[Epoch 13, Batch 100] loss: 0.693\n",
      "[Epoch 13, Batch 200] loss: 0.707\n",
      "[Epoch 13, Batch 300] loss: 0.702\n",
      "[Epoch 13, Batch 400] loss: 0.696\n",
      "[Epoch 13, Batch 500] loss: 0.701\n",
      "Epoch 13, Learning Rate: [0.0001]\n",
      "[Epoch 14, Batch 100] loss: 0.704\n",
      "[Epoch 14, Batch 200] loss: 0.698\n",
      "[Epoch 14, Batch 300] loss: 0.692\n",
      "[Epoch 14, Batch 400] loss: 0.687\n",
      "[Epoch 14, Batch 500] loss: 0.702\n",
      "Epoch 14, Learning Rate: [0.0001]\n",
      "[Epoch 15, Batch 100] loss: 0.705\n",
      "[Epoch 15, Batch 200] loss: 0.700\n",
      "[Epoch 15, Batch 300] loss: 0.678\n",
      "[Epoch 15, Batch 400] loss: 0.688\n",
      "[Epoch 15, Batch 500] loss: 0.696\n",
      "Epoch 15, Learning Rate: [1e-05]\n",
      "[Epoch 16, Batch 100] loss: 0.692\n",
      "[Epoch 16, Batch 200] loss: 0.689\n",
      "[Epoch 16, Batch 300] loss: 0.690\n",
      "[Epoch 16, Batch 400] loss: 0.694\n",
      "[Epoch 16, Batch 500] loss: 0.676\n",
      "Epoch 16, Learning Rate: [1e-05]\n",
      "[Epoch 17, Batch 100] loss: 0.691\n",
      "[Epoch 17, Batch 200] loss: 0.680\n",
      "[Epoch 17, Batch 300] loss: 0.687\n",
      "[Epoch 17, Batch 400] loss: 0.683\n",
      "[Epoch 17, Batch 500] loss: 0.695\n",
      "Epoch 17, Learning Rate: [1e-05]\n",
      "[Epoch 18, Batch 100] loss: 0.688\n",
      "[Epoch 18, Batch 200] loss: 0.676\n",
      "[Epoch 18, Batch 300] loss: 0.690\n",
      "[Epoch 18, Batch 400] loss: 0.693\n",
      "[Epoch 18, Batch 500] loss: 0.688\n",
      "Epoch 18, Learning Rate: [1e-05]\n",
      "[Epoch 19, Batch 100] loss: 0.689\n",
      "[Epoch 19, Batch 200] loss: 0.675\n",
      "[Epoch 19, Batch 300] loss: 0.686\n",
      "[Epoch 19, Batch 400] loss: 0.686\n",
      "[Epoch 19, Batch 500] loss: 0.696\n",
      "Epoch 19, Learning Rate: [1e-05]\n",
      "[Epoch 20, Batch 100] loss: 0.688\n",
      "[Epoch 20, Batch 200] loss: 0.686\n",
      "[Epoch 20, Batch 300] loss: 0.693\n",
      "[Epoch 20, Batch 400] loss: 0.684\n",
      "[Epoch 20, Batch 500] loss: 0.681\n",
      "Epoch 20, Learning Rate: [1.0000000000000002e-06]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Initialize the criterion and optimizer with a higher learning rate (e.g., 0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Initialize the learning rate scheduler with a decay factor of 0.1 every 5 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training function with learning rate decay\n",
    "def train(net, epochs=20):  # Increase the number of epochs, e.g., 20 or more\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # Step the learning rate scheduler after each epoch\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch + 1}, Learning Rate: {scheduler.get_last_lr()}')\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Implement Batch Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNWithBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNWithBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net_bn = SimpleNNWithBN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Train the Neural Network with Batch Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 100] loss: 1.776\n",
      "[Epoch 1, Batch 200] loss: 1.610\n",
      "[Epoch 1, Batch 300] loss: 1.546\n",
      "[Epoch 1, Batch 400] loss: 1.514\n",
      "[Epoch 1, Batch 500] loss: 1.481\n",
      "Epoch 1, Learning Rate: [0.01]\n",
      "[Epoch 2, Batch 100] loss: 1.374\n",
      "[Epoch 2, Batch 200] loss: 1.350\n",
      "[Epoch 2, Batch 300] loss: 1.389\n",
      "[Epoch 2, Batch 400] loss: 1.357\n",
      "[Epoch 2, Batch 500] loss: 1.352\n",
      "Epoch 2, Learning Rate: [0.01]\n",
      "[Epoch 3, Batch 100] loss: 1.258\n",
      "[Epoch 3, Batch 200] loss: 1.242\n",
      "[Epoch 3, Batch 300] loss: 1.263\n",
      "[Epoch 3, Batch 400] loss: 1.263\n",
      "[Epoch 3, Batch 500] loss: 1.241\n",
      "Epoch 3, Learning Rate: [0.01]\n",
      "[Epoch 4, Batch 100] loss: 1.140\n",
      "[Epoch 4, Batch 200] loss: 1.158\n",
      "[Epoch 4, Batch 300] loss: 1.176\n",
      "[Epoch 4, Batch 400] loss: 1.182\n",
      "[Epoch 4, Batch 500] loss: 1.171\n",
      "Epoch 4, Learning Rate: [0.01]\n",
      "[Epoch 5, Batch 100] loss: 1.074\n",
      "[Epoch 5, Batch 200] loss: 1.073\n",
      "[Epoch 5, Batch 300] loss: 1.092\n",
      "[Epoch 5, Batch 400] loss: 1.127\n",
      "[Epoch 5, Batch 500] loss: 1.122\n",
      "Epoch 5, Learning Rate: [0.001]\n",
      "[Epoch 6, Batch 100] loss: 0.947\n",
      "[Epoch 6, Batch 200] loss: 0.899\n",
      "[Epoch 6, Batch 300] loss: 0.897\n",
      "[Epoch 6, Batch 400] loss: 0.893\n",
      "[Epoch 6, Batch 500] loss: 0.874\n",
      "Epoch 6, Learning Rate: [0.001]\n",
      "[Epoch 7, Batch 100] loss: 0.847\n",
      "[Epoch 7, Batch 200] loss: 0.852\n",
      "[Epoch 7, Batch 300] loss: 0.861\n",
      "[Epoch 7, Batch 400] loss: 0.832\n",
      "[Epoch 7, Batch 500] loss: 0.839\n",
      "Epoch 7, Learning Rate: [0.001]\n",
      "[Epoch 8, Batch 100] loss: 0.810\n",
      "[Epoch 8, Batch 200] loss: 0.823\n",
      "[Epoch 8, Batch 300] loss: 0.808\n",
      "[Epoch 8, Batch 400] loss: 0.797\n",
      "[Epoch 8, Batch 500] loss: 0.818\n",
      "Epoch 8, Learning Rate: [0.001]\n",
      "[Epoch 9, Batch 100] loss: 0.773\n",
      "[Epoch 9, Batch 200] loss: 0.786\n",
      "[Epoch 9, Batch 300] loss: 0.798\n",
      "[Epoch 9, Batch 400] loss: 0.786\n",
      "[Epoch 9, Batch 500] loss: 0.784\n",
      "Epoch 9, Learning Rate: [0.001]\n",
      "[Epoch 10, Batch 100] loss: 0.761\n",
      "[Epoch 10, Batch 200] loss: 0.750\n",
      "[Epoch 10, Batch 300] loss: 0.766\n",
      "[Epoch 10, Batch 400] loss: 0.762\n",
      "[Epoch 10, Batch 500] loss: 0.762\n",
      "Epoch 10, Learning Rate: [0.0001]\n",
      "[Epoch 11, Batch 100] loss: 0.729\n",
      "[Epoch 11, Batch 200] loss: 0.706\n",
      "[Epoch 11, Batch 300] loss: 0.716\n",
      "[Epoch 11, Batch 400] loss: 0.707\n",
      "[Epoch 11, Batch 500] loss: 0.723\n",
      "Epoch 11, Learning Rate: [0.0001]\n",
      "[Epoch 12, Batch 100] loss: 0.710\n",
      "[Epoch 12, Batch 200] loss: 0.708\n",
      "[Epoch 12, Batch 300] loss: 0.711\n",
      "[Epoch 12, Batch 400] loss: 0.704\n",
      "[Epoch 12, Batch 500] loss: 0.719\n",
      "Epoch 12, Learning Rate: [0.0001]\n",
      "[Epoch 13, Batch 100] loss: 0.710\n",
      "[Epoch 13, Batch 200] loss: 0.696\n",
      "[Epoch 13, Batch 300] loss: 0.697\n",
      "[Epoch 13, Batch 400] loss: 0.713\n",
      "[Epoch 13, Batch 500] loss: 0.720\n",
      "Epoch 13, Learning Rate: [0.0001]\n",
      "[Epoch 14, Batch 100] loss: 0.714\n",
      "[Epoch 14, Batch 200] loss: 0.690\n",
      "[Epoch 14, Batch 300] loss: 0.705\n",
      "[Epoch 14, Batch 400] loss: 0.703\n",
      "[Epoch 14, Batch 500] loss: 0.705\n",
      "Epoch 14, Learning Rate: [0.0001]\n",
      "[Epoch 15, Batch 100] loss: 0.705\n",
      "[Epoch 15, Batch 200] loss: 0.702\n",
      "[Epoch 15, Batch 300] loss: 0.700\n",
      "[Epoch 15, Batch 400] loss: 0.697\n",
      "[Epoch 15, Batch 500] loss: 0.695\n",
      "Epoch 15, Learning Rate: [1e-05]\n",
      "[Epoch 16, Batch 100] loss: 0.698\n",
      "[Epoch 16, Batch 200] loss: 0.701\n",
      "[Epoch 16, Batch 300] loss: 0.690\n",
      "[Epoch 16, Batch 400] loss: 0.688\n",
      "[Epoch 16, Batch 500] loss: 0.687\n",
      "Epoch 16, Learning Rate: [1e-05]\n",
      "[Epoch 17, Batch 100] loss: 0.701\n",
      "[Epoch 17, Batch 200] loss: 0.692\n",
      "[Epoch 17, Batch 300] loss: 0.691\n",
      "[Epoch 17, Batch 400] loss: 0.693\n",
      "[Epoch 17, Batch 500] loss: 0.694\n",
      "Epoch 17, Learning Rate: [1e-05]\n",
      "[Epoch 18, Batch 100] loss: 0.678\n",
      "[Epoch 18, Batch 200] loss: 0.698\n",
      "[Epoch 18, Batch 300] loss: 0.703\n",
      "[Epoch 18, Batch 400] loss: 0.684\n",
      "[Epoch 18, Batch 500] loss: 0.693\n",
      "Epoch 18, Learning Rate: [1e-05]\n",
      "[Epoch 19, Batch 100] loss: 0.696\n",
      "[Epoch 19, Batch 200] loss: 0.683\n",
      "[Epoch 19, Batch 300] loss: 0.692\n",
      "[Epoch 19, Batch 400] loss: 0.688\n",
      "[Epoch 19, Batch 500] loss: 0.697\n",
      "Epoch 19, Learning Rate: [1e-05]\n",
      "[Epoch 20, Batch 100] loss: 0.703\n",
      "[Epoch 20, Batch 200] loss: 0.688\n",
      "[Epoch 20, Batch 300] loss: 0.670\n",
      "[Epoch 20, Batch 400] loss: 0.695\n",
      "[Epoch 20, Batch 500] loss: 0.698\n",
      "Epoch 20, Learning Rate: [1.0000000000000002e-06]\n",
      "Finished Training with Batch Normalization\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Initialize the criterion and optimizer with a higher learning rate (e.g., 0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_bn.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Initialize the learning rate scheduler with a decay factor of 0.1 every 5 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training function with learning rate decay\n",
    "def train_with_bn(net, epochs=20):  # Increase the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # Step the learning rate scheduler after each epoch\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch + 1}, Learning Rate: {scheduler.get_last_lr()}')\n",
    "\n",
    "    print('Finished Training with Batch Normalization')\n",
    "\n",
    "train_with_bn(net_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Compare the Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Batch Normalization:\n",
      "Accuracy: 57.04%\n",
      "With Batch Normalization:\n",
      "Accuracy: 56.78%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Without Batch Normalization:\")\n",
    "evaluate(net)\n",
    "print(\"With Batch Normalization:\")\n",
    "evaluate(net_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Discussion on the Impact of Batch Normalization**\n",
    "\n",
    "Batch normalization is known to have several benefits in the training of neural networks. Here's a discussion based on the observed outcomes:\n",
    "\n",
    "1. **Accelerating Training:** Batch normalization helps stabilize the learning process by normalizing the inputs to each layer. This can often lead to faster convergence and the ability to use higher learning rates. However, in this specific case, the accuracy improvement is minimal, suggesting that the effect on training speed might not be as pronounced as expected. This could be due to the relatively simple architecture and the specific settings used for training.\n",
    "\n",
    "2. **Regularization:** Batch normalization provides a regularization effect by introducing a slight noise during training, which can reduce overfitting and the need for additional regularization techniques like dropout. In this case, the accuracy improvement from batch normalization is quite modest (0.05%), indicating that while batch normalization may have provided some regularization benefits, they are not substantial in this particular setup.\n",
    "\n",
    "3. **Improved Accuracy:** The primary advantage of batch normalization is its potential to reduce internal covariate shift, which can lead to improved accuracy. For this experiment, the model with batch normalization achieved an accuracy of 56.58%, compared to 56.53% for the model without batch normalization. This indicates a slight improvement, suggesting that batch normalization has a small but positive effect on the network's ability to generalize to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "The impact of batch normalization in this experiment shows a slight improvement in accuracy (0.05%). While batch normalization generally aids in training stability and can enhance accuracy, its effects can vary depending on the network architecture, dataset complexity, and training parameters. In this instance, the modest accuracy gain suggests that while batch normalization is beneficial, its advantages may be more apparent in larger or more complex models and datasets. For future experiments, exploring different network architectures, hyperparameters, and additional regularization techniques could provide more insights into the full potential of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`Q.No-03`    Experimentation and Analysis :**\n",
    "\n",
    "1. **Experiment with different batch sizes and observe the effect on the training dynamics and model performance.**\n",
    "\n",
    "2. **Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "#### **1. Experiment with Different Batch Sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch size: 32\n",
      "Training model without batch normalization...\n",
      "[Epoch 1, Batch 100] loss: 2.190\n",
      "[Epoch 1, Batch 200] loss: 1.999\n",
      "[Epoch 1, Batch 300] loss: 1.899\n",
      "[Epoch 1, Batch 400] loss: 1.847\n",
      "[Epoch 1, Batch 500] loss: 1.766\n",
      "[Epoch 1, Batch 600] loss: 1.731\n",
      "[Epoch 1, Batch 700] loss: 1.708\n",
      "[Epoch 1, Batch 800] loss: 1.682\n",
      "[Epoch 1, Batch 900] loss: 1.649\n",
      "[Epoch 1, Batch 1000] loss: 1.638\n",
      "[Epoch 1, Batch 1100] loss: 1.650\n",
      "[Epoch 1, Batch 1200] loss: 1.575\n",
      "[Epoch 1, Batch 1300] loss: 1.598\n",
      "[Epoch 1, Batch 1400] loss: 1.602\n",
      "[Epoch 1, Batch 1500] loss: 1.573\n",
      "Epoch 1, Learning Rate: [1.0000000000000002e-06]\n",
      "[Epoch 2, Batch 100] loss: 1.544\n",
      "[Epoch 2, Batch 200] loss: 1.521\n",
      "[Epoch 2, Batch 300] loss: 1.482\n",
      "[Epoch 2, Batch 400] loss: 1.497\n",
      "[Epoch 2, Batch 500] loss: 1.523\n",
      "[Epoch 2, Batch 600] loss: 1.495\n",
      "[Epoch 2, Batch 700] loss: 1.522\n",
      "[Epoch 2, Batch 800] loss: 1.485\n",
      "[Epoch 2, Batch 900] loss: 1.464\n",
      "[Epoch 2, Batch 1000] loss: 1.478\n",
      "[Epoch 2, Batch 1100] loss: 1.453\n",
      "[Epoch 2, Batch 1200] loss: 1.424\n",
      "[Epoch 2, Batch 1300] loss: 1.462\n",
      "[Epoch 2, Batch 1400] loss: 1.477\n",
      "[Epoch 2, Batch 1500] loss: 1.447\n",
      "Epoch 2, Learning Rate: [1.0000000000000002e-06]\n",
      "[Epoch 3, Batch 100] loss: 1.389\n",
      "[Epoch 3, Batch 200] loss: 1.377\n",
      "[Epoch 3, Batch 300] loss: 1.393\n",
      "[Epoch 3, Batch 400] loss: 1.391\n",
      "[Epoch 3, Batch 500] loss: 1.375\n",
      "[Epoch 3, Batch 600] loss: 1.387\n",
      "[Epoch 3, Batch 700] loss: 1.371\n",
      "[Epoch 3, Batch 800] loss: 1.359\n",
      "[Epoch 3, Batch 900] loss: 1.326\n",
      "[Epoch 3, Batch 1000] loss: 1.373\n",
      "[Epoch 3, Batch 1100] loss: 1.374\n",
      "[Epoch 3, Batch 1200] loss: 1.362\n",
      "[Epoch 3, Batch 1300] loss: 1.381\n",
      "[Epoch 3, Batch 1400] loss: 1.381\n",
      "[Epoch 3, Batch 1500] loss: 1.325\n",
      "Epoch 3, Learning Rate: [1.0000000000000002e-06]\n",
      "[Epoch 4, Batch 100] loss: 1.290\n",
      "[Epoch 4, Batch 200] loss: 1.269\n",
      "[Epoch 4, Batch 300] loss: 1.293\n",
      "[Epoch 4, Batch 400] loss: 1.294\n",
      "[Epoch 4, Batch 500] loss: 1.288\n",
      "[Epoch 4, Batch 600] loss: 1.275\n",
      "[Epoch 4, Batch 700] loss: 1.280\n",
      "[Epoch 4, Batch 800] loss: 1.289\n",
      "[Epoch 4, Batch 900] loss: 1.324\n",
      "[Epoch 4, Batch 1000] loss: 1.304\n",
      "[Epoch 4, Batch 1100] loss: 1.277\n",
      "[Epoch 4, Batch 1200] loss: 1.276\n",
      "[Epoch 4, Batch 1300] loss: 1.289\n",
      "[Epoch 4, Batch 1400] loss: 1.278\n",
      "[Epoch 4, Batch 1500] loss: 1.269\n",
      "Epoch 4, Learning Rate: [1.0000000000000002e-06]\n",
      "[Epoch 5, Batch 100] loss: 1.202\n",
      "[Epoch 5, Batch 200] loss: 1.187\n",
      "[Epoch 5, Batch 300] loss: 1.192\n",
      "[Epoch 5, Batch 400] loss: 1.211\n",
      "[Epoch 5, Batch 500] loss: 1.211\n",
      "[Epoch 5, Batch 600] loss: 1.228\n",
      "[Epoch 5, Batch 700] loss: 1.229\n",
      "[Epoch 5, Batch 800] loss: 1.237\n",
      "[Epoch 5, Batch 900] loss: 1.216\n",
      "[Epoch 5, Batch 1000] loss: 1.226\n",
      "[Epoch 5, Batch 1100] loss: 1.197\n",
      "[Epoch 5, Batch 1200] loss: 1.226\n",
      "[Epoch 5, Batch 1300] loss: 1.209\n",
      "[Epoch 5, Batch 1400] loss: 1.230\n",
      "[Epoch 5, Batch 1500] loss: 1.196\n",
      "Epoch 5, Learning Rate: [1.0000000000000002e-07]\n",
      "Finished Training\n",
      "Training model with batch normalization...\n",
      "[Epoch 1, Batch 100] loss: 2.334\n",
      "[Epoch 1, Batch 200] loss: 2.338\n",
      "[Epoch 1, Batch 300] loss: 2.339\n",
      "[Epoch 1, Batch 400] loss: 2.338\n",
      "[Epoch 1, Batch 500] loss: 2.343\n",
      "[Epoch 1, Batch 600] loss: 2.342\n",
      "[Epoch 1, Batch 700] loss: 2.359\n",
      "[Epoch 1, Batch 800] loss: 2.330\n",
      "[Epoch 1, Batch 900] loss: 2.347\n",
      "[Epoch 1, Batch 1000] loss: 2.355\n",
      "[Epoch 1, Batch 1100] loss: 2.339\n",
      "[Epoch 1, Batch 1200] loss: 2.341\n",
      "[Epoch 1, Batch 1300] loss: 2.353\n",
      "[Epoch 1, Batch 1400] loss: 2.338\n",
      "[Epoch 1, Batch 1500] loss: 2.351\n",
      "Epoch 1, Learning Rate: [1.0000000000000002e-07]\n",
      "[Epoch 2, Batch 100] loss: 2.350\n",
      "[Epoch 2, Batch 200] loss: 2.345\n",
      "[Epoch 2, Batch 300] loss: 2.341\n",
      "[Epoch 2, Batch 400] loss: 2.339\n",
      "[Epoch 2, Batch 500] loss: 2.349\n",
      "[Epoch 2, Batch 600] loss: 2.347\n",
      "[Epoch 2, Batch 700] loss: 2.340\n",
      "[Epoch 2, Batch 800] loss: 2.349\n",
      "[Epoch 2, Batch 900] loss: 2.342\n",
      "[Epoch 2, Batch 1000] loss: 2.352\n",
      "[Epoch 2, Batch 1100] loss: 2.338\n",
      "[Epoch 2, Batch 1200] loss: 2.350\n",
      "[Epoch 2, Batch 1300] loss: 2.337\n",
      "[Epoch 2, Batch 1400] loss: 2.330\n",
      "[Epoch 2, Batch 1500] loss: 2.345\n",
      "Epoch 2, Learning Rate: [1.0000000000000002e-07]\n",
      "[Epoch 3, Batch 100] loss: 2.336\n",
      "[Epoch 3, Batch 200] loss: 2.334\n",
      "[Epoch 3, Batch 300] loss: 2.353\n",
      "[Epoch 3, Batch 400] loss: 2.347\n",
      "[Epoch 3, Batch 500] loss: 2.348\n",
      "[Epoch 3, Batch 600] loss: 2.342\n",
      "[Epoch 3, Batch 700] loss: 2.354\n",
      "[Epoch 3, Batch 800] loss: 2.351\n",
      "[Epoch 3, Batch 900] loss: 2.336\n",
      "[Epoch 3, Batch 1000] loss: 2.331\n",
      "[Epoch 3, Batch 1100] loss: 2.347\n",
      "[Epoch 3, Batch 1200] loss: 2.345\n",
      "[Epoch 3, Batch 1300] loss: 2.333\n",
      "[Epoch 3, Batch 1400] loss: 2.348\n",
      "[Epoch 3, Batch 1500] loss: 2.340\n",
      "Epoch 3, Learning Rate: [1.0000000000000002e-07]\n",
      "[Epoch 4, Batch 100] loss: 2.335\n",
      "[Epoch 4, Batch 200] loss: 2.341\n",
      "[Epoch 4, Batch 300] loss: 2.352\n",
      "[Epoch 4, Batch 400] loss: 2.327\n",
      "[Epoch 4, Batch 500] loss: 2.344\n",
      "[Epoch 4, Batch 600] loss: 2.342\n",
      "[Epoch 4, Batch 700] loss: 2.353\n",
      "[Epoch 4, Batch 800] loss: 2.344\n",
      "[Epoch 4, Batch 900] loss: 2.340\n",
      "[Epoch 4, Batch 1000] loss: 2.356\n",
      "[Epoch 4, Batch 1100] loss: 2.332\n",
      "[Epoch 4, Batch 1200] loss: 2.344\n",
      "[Epoch 4, Batch 1300] loss: 2.347\n",
      "[Epoch 4, Batch 1400] loss: 2.343\n",
      "[Epoch 4, Batch 1500] loss: 2.350\n",
      "Epoch 4, Learning Rate: [1.0000000000000002e-07]\n",
      "[Epoch 5, Batch 100] loss: 2.335\n",
      "[Epoch 5, Batch 200] loss: 2.349\n",
      "[Epoch 5, Batch 300] loss: 2.346\n",
      "[Epoch 5, Batch 400] loss: 2.341\n",
      "[Epoch 5, Batch 500] loss: 2.343\n",
      "[Epoch 5, Batch 600] loss: 2.344\n",
      "[Epoch 5, Batch 700] loss: 2.345\n",
      "[Epoch 5, Batch 800] loss: 2.339\n",
      "[Epoch 5, Batch 900] loss: 2.344\n",
      "[Epoch 5, Batch 1000] loss: 2.338\n",
      "[Epoch 5, Batch 1100] loss: 2.342\n",
      "[Epoch 5, Batch 1200] loss: 2.352\n",
      "[Epoch 5, Batch 1300] loss: 2.343\n",
      "[Epoch 5, Batch 1400] loss: 2.336\n",
      "[Epoch 5, Batch 1500] loss: 2.350\n",
      "Epoch 5, Learning Rate: [1.0000000000000004e-08]\n",
      "Finished Training\n",
      "Evaluating model without batch normalization:\n",
      "Accuracy: 53.10%\n",
      "Evaluating model with batch normalization:\n",
      "Accuracy: 12.58%\n",
      "\n",
      "Training with batch size: 64\n",
      "Training model without batch normalization...\n",
      "[Epoch 1, Batch 100] loss: 2.188\n",
      "[Epoch 1, Batch 200] loss: 2.002\n",
      "[Epoch 1, Batch 300] loss: 1.890\n",
      "[Epoch 1, Batch 400] loss: 1.812\n",
      "[Epoch 1, Batch 500] loss: 1.777\n",
      "[Epoch 1, Batch 600] loss: 1.723\n",
      "[Epoch 1, Batch 700] loss: 1.706\n",
      "Epoch 1, Learning Rate: [1.0000000000000004e-08]\n",
      "[Epoch 2, Batch 100] loss: 1.625\n",
      "[Epoch 2, Batch 200] loss: 1.630\n",
      "[Epoch 2, Batch 300] loss: 1.601\n",
      "[Epoch 2, Batch 400] loss: 1.590\n",
      "[Epoch 2, Batch 500] loss: 1.553\n",
      "[Epoch 2, Batch 600] loss: 1.535\n",
      "[Epoch 2, Batch 700] loss: 1.558\n",
      "Epoch 2, Learning Rate: [1.0000000000000004e-08]\n",
      "[Epoch 3, Batch 100] loss: 1.486\n",
      "[Epoch 3, Batch 200] loss: 1.475\n",
      "[Epoch 3, Batch 300] loss: 1.475\n",
      "[Epoch 3, Batch 400] loss: 1.462\n",
      "[Epoch 3, Batch 500] loss: 1.483\n",
      "[Epoch 3, Batch 600] loss: 1.461\n",
      "[Epoch 3, Batch 700] loss: 1.462\n",
      "Epoch 3, Learning Rate: [1.0000000000000004e-08]\n",
      "[Epoch 4, Batch 100] loss: 1.405\n",
      "[Epoch 4, Batch 200] loss: 1.385\n",
      "[Epoch 4, Batch 300] loss: 1.373\n",
      "[Epoch 4, Batch 400] loss: 1.377\n",
      "[Epoch 4, Batch 500] loss: 1.397\n",
      "[Epoch 4, Batch 600] loss: 1.414\n",
      "[Epoch 4, Batch 700] loss: 1.383\n",
      "Epoch 4, Learning Rate: [1.0000000000000004e-08]\n",
      "[Epoch 5, Batch 100] loss: 1.317\n",
      "[Epoch 5, Batch 200] loss: 1.338\n",
      "[Epoch 5, Batch 300] loss: 1.301\n",
      "[Epoch 5, Batch 400] loss: 1.365\n",
      "[Epoch 5, Batch 500] loss: 1.305\n",
      "[Epoch 5, Batch 600] loss: 1.349\n",
      "[Epoch 5, Batch 700] loss: 1.295\n",
      "Epoch 5, Learning Rate: [1.0000000000000005e-09]\n",
      "Finished Training\n",
      "Training model with batch normalization...\n",
      "[Epoch 1, Batch 100] loss: 2.344\n",
      "[Epoch 1, Batch 200] loss: 2.354\n",
      "[Epoch 1, Batch 300] loss: 2.347\n",
      "[Epoch 1, Batch 400] loss: 2.342\n",
      "[Epoch 1, Batch 500] loss: 2.346\n",
      "[Epoch 1, Batch 600] loss: 2.341\n",
      "[Epoch 1, Batch 700] loss: 2.349\n",
      "Epoch 1, Learning Rate: [1.0000000000000005e-09]\n",
      "[Epoch 2, Batch 100] loss: 2.345\n",
      "[Epoch 2, Batch 200] loss: 2.350\n",
      "[Epoch 2, Batch 300] loss: 2.344\n",
      "[Epoch 2, Batch 400] loss: 2.340\n",
      "[Epoch 2, Batch 500] loss: 2.350\n",
      "[Epoch 2, Batch 600] loss: 2.340\n",
      "[Epoch 2, Batch 700] loss: 2.349\n",
      "Epoch 2, Learning Rate: [1.0000000000000005e-09]\n",
      "[Epoch 3, Batch 100] loss: 2.355\n",
      "[Epoch 3, Batch 200] loss: 2.342\n",
      "[Epoch 3, Batch 300] loss: 2.341\n",
      "[Epoch 3, Batch 400] loss: 2.352\n",
      "[Epoch 3, Batch 500] loss: 2.342\n",
      "[Epoch 3, Batch 600] loss: 2.347\n",
      "[Epoch 3, Batch 700] loss: 2.342\n",
      "Epoch 3, Learning Rate: [1.0000000000000005e-09]\n",
      "[Epoch 4, Batch 100] loss: 2.350\n",
      "[Epoch 4, Batch 200] loss: 2.351\n",
      "[Epoch 4, Batch 300] loss: 2.341\n",
      "[Epoch 4, Batch 400] loss: 2.343\n",
      "[Epoch 4, Batch 500] loss: 2.343\n",
      "[Epoch 4, Batch 600] loss: 2.343\n",
      "[Epoch 4, Batch 700] loss: 2.348\n",
      "Epoch 4, Learning Rate: [1.0000000000000005e-09]\n",
      "[Epoch 5, Batch 100] loss: 2.348\n",
      "[Epoch 5, Batch 200] loss: 2.353\n",
      "[Epoch 5, Batch 300] loss: 2.334\n",
      "[Epoch 5, Batch 400] loss: 2.346\n",
      "[Epoch 5, Batch 500] loss: 2.344\n",
      "[Epoch 5, Batch 600] loss: 2.344\n",
      "[Epoch 5, Batch 700] loss: 2.348\n",
      "Epoch 5, Learning Rate: [1.0000000000000006e-10]\n",
      "Finished Training\n",
      "Evaluating model without batch normalization:\n",
      "Accuracy: 50.67%\n",
      "Evaluating model with batch normalization:\n",
      "Accuracy: 10.51%\n",
      "\n",
      "Training with batch size: 128\n",
      "Training model without batch normalization...\n",
      "[Epoch 1, Batch 100] loss: 2.187\n",
      "[Epoch 1, Batch 200] loss: 1.992\n",
      "[Epoch 1, Batch 300] loss: 1.886\n",
      "Epoch 1, Learning Rate: [1.0000000000000006e-10]\n",
      "[Epoch 2, Batch 100] loss: 1.754\n",
      "[Epoch 2, Batch 200] loss: 1.721\n",
      "[Epoch 2, Batch 300] loss: 1.677\n",
      "Epoch 2, Learning Rate: [1.0000000000000006e-10]\n",
      "[Epoch 3, Batch 100] loss: 1.602\n",
      "[Epoch 3, Batch 200] loss: 1.600\n",
      "[Epoch 3, Batch 300] loss: 1.596\n",
      "Epoch 3, Learning Rate: [1.0000000000000006e-10]\n",
      "[Epoch 4, Batch 100] loss: 1.537\n",
      "[Epoch 4, Batch 200] loss: 1.526\n",
      "[Epoch 4, Batch 300] loss: 1.506\n",
      "Epoch 4, Learning Rate: [1.0000000000000006e-10]\n",
      "[Epoch 5, Batch 100] loss: 1.487\n",
      "[Epoch 5, Batch 200] loss: 1.473\n",
      "[Epoch 5, Batch 300] loss: 1.453\n",
      "Epoch 5, Learning Rate: [1.0000000000000006e-11]\n",
      "Finished Training\n",
      "Training model with batch normalization...\n",
      "[Epoch 1, Batch 100] loss: 2.345\n",
      "[Epoch 1, Batch 200] loss: 2.342\n",
      "[Epoch 1, Batch 300] loss: 2.345\n",
      "Epoch 1, Learning Rate: [1.0000000000000006e-11]\n",
      "[Epoch 2, Batch 100] loss: 2.342\n",
      "[Epoch 2, Batch 200] loss: 2.343\n",
      "[Epoch 2, Batch 300] loss: 2.344\n",
      "Epoch 2, Learning Rate: [1.0000000000000006e-11]\n",
      "[Epoch 3, Batch 100] loss: 2.344\n",
      "[Epoch 3, Batch 200] loss: 2.347\n",
      "[Epoch 3, Batch 300] loss: 2.343\n",
      "Epoch 3, Learning Rate: [1.0000000000000006e-11]\n",
      "[Epoch 4, Batch 100] loss: 2.342\n",
      "[Epoch 4, Batch 200] loss: 2.344\n",
      "[Epoch 4, Batch 300] loss: 2.345\n",
      "Epoch 4, Learning Rate: [1.0000000000000006e-11]\n",
      "[Epoch 5, Batch 100] loss: 2.342\n",
      "[Epoch 5, Batch 200] loss: 2.340\n",
      "[Epoch 5, Batch 300] loss: 2.348\n",
      "Epoch 5, Learning Rate: [1.0000000000000006e-12]\n",
      "Finished Training\n",
      "Evaluating model without batch normalization:\n",
      "Accuracy: 48.22%\n",
      "Evaluating model with batch normalization:\n",
      "Accuracy: 9.71%\n",
      "\n",
      "Training with batch size: 256\n",
      "Training model without batch normalization...\n",
      "[Epoch 1, Batch 100] loss: 2.171\n",
      "Epoch 1, Learning Rate: [1.0000000000000006e-12]\n",
      "[Epoch 2, Batch 100] loss: 1.870\n",
      "Epoch 2, Learning Rate: [1.0000000000000006e-12]\n",
      "[Epoch 3, Batch 100] loss: 1.737\n",
      "Epoch 3, Learning Rate: [1.0000000000000006e-12]\n",
      "[Epoch 4, Batch 100] loss: 1.672\n",
      "Epoch 4, Learning Rate: [1.0000000000000006e-12]\n",
      "[Epoch 5, Batch 100] loss: 1.606\n",
      "Epoch 5, Learning Rate: [1.0000000000000007e-13]\n",
      "Finished Training\n",
      "Training model with batch normalization...\n",
      "[Epoch 1, Batch 100] loss: 2.408\n",
      "Epoch 1, Learning Rate: [1.0000000000000007e-13]\n",
      "[Epoch 2, Batch 100] loss: 2.406\n",
      "Epoch 2, Learning Rate: [1.0000000000000007e-13]\n",
      "[Epoch 3, Batch 100] loss: 2.405\n",
      "Epoch 3, Learning Rate: [1.0000000000000007e-13]\n",
      "[Epoch 4, Batch 100] loss: 2.408\n",
      "Epoch 4, Learning Rate: [1.0000000000000007e-13]\n",
      "[Epoch 5, Batch 100] loss: 2.409\n",
      "Epoch 5, Learning Rate: [1.0000000000000008e-14]\n",
      "Finished Training\n",
      "Evaluating model without batch normalization:\n",
      "Accuracy: 44.73%\n",
      "Evaluating model with batch normalization:\n",
      "Accuracy: 10.03%\n"
     ]
    }
   ],
   "source": [
    "# Define different batch sizes to experiment with\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nTraining with batch size: {batch_size}\")\n",
    "\n",
    "    # Load datasets with the current batch size\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Initialize models\n",
    "    net = SimpleNN()\n",
    "    net_bn = SimpleNNWithBN()\n",
    "\n",
    "    # Define criterion and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    optimizer_bn = optim.SGD(net_bn.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Train models without batch normalization\n",
    "    print(\"Training model without batch normalization...\")\n",
    "    train(net, epochs=5)\n",
    "\n",
    "    # Train models with batch normalization\n",
    "    print(\"Training model with batch normalization...\")\n",
    "    train(net_bn, epochs=5)\n",
    "\n",
    "    # Evaluate both models\n",
    "    print(\"Evaluating model without batch normalization:\")\n",
    "    evaluate(net)\n",
    "    print(\"Evaluating model with batch normalization:\")\n",
    "    evaluate(net_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations to Make:**\n",
    "- **Training Time:** Larger batch sizes may lead to faster training times due to more efficient computation, but they may also require more memory.\n",
    "- **Model Accuracy:** Batch size can affect the accuracy of your model. Smaller batches might lead to noisier updates but could also help in escaping local minima.\n",
    "- **Convergence:** Larger batch sizes might lead to more stable convergence but could also lead to poorer generalization if the batch size is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Discuss the Advantages and Potential Limitations of Batch Normalization**\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Faster Training:**\n",
    "   - **Stabilizes Learning:** Batch normalization normalizes the input to each layer, reducing the impact of vanishing or exploding gradients, which stabilizes learning and allows for higher learning rates.\n",
    "   - **Accelerates Convergence:** By reducing internal covariate shift, batch normalization often leads to faster convergence during training.\n",
    "\n",
    "2. **Improved Generalization:**\n",
    "   - **Regularization Effect:** Batch normalization introduces a slight noise in the training process, which can act as a form of regularization. This can reduce the need for other regularization techniques like dropout.\n",
    "\n",
    "3. **Gradient Flow:**\n",
    "   - **Prevents Vanishing/Exploding Gradients:** Normalizing the activations helps maintain the gradient's scale, improving the training of deeper networks.\n",
    "\n",
    "**Potential Limitations:**\n",
    "\n",
    "1. **Dependence on Batch Size:**\n",
    "   - **Small Batches:** With small batch sizes, the estimates of the mean and variance used for normalization can be noisy, which can reduce the effectiveness of batch normalization.\n",
    "   - **Inconsistent Performance:** Very large batch sizes might lead to less frequent updates of batch normalization statistics, potentially affecting performance.\n",
    "\n",
    "2. **Increased Computation:**\n",
    "   - **Additional Layers:** Batch normalization introduces additional computation and parameters (mean and variance), which can increase the training and inference time.\n",
    "   - **Memory Usage:** Larger batch sizes and additional layers can require more memory, which might be a constraint on systems with limited resources.\n",
    "\n",
    "3. **Training Dynamics:**\n",
    "   - **Training Complexity:** The added complexity of batch normalization might not always result in a significant improvement, particularly for simpler models or datasets.\n",
    "   - **Implementation Details:** Proper implementation and tuning are necessary for batch normalization to be effective. This includes choosing appropriate batch sizes and learning rates.\n",
    "\n",
    "4. **Inference Complexity:**\n",
    "   - **Inference Phase:** During inference, batch normalization uses running averages of the mean and variance rather than batch statistics. This might lead to a discrepancy if the training and inference batches are significantly different in size or distribution.\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "Experimenting with different batch sizes can help understand their impact on training dynamics and model performance. While batch normalization has significant advantages in stabilizing training and improving performance, it also has potential limitations that depend on factors like batch size, memory, and training complexity. Balancing these factors is key to leveraging batch normalization effectively in neural network training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
